{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt2c5Z3WIHvS",
        "outputId": "02d71f1d-5da4-4f7f-afc9-1aeca6114fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "pip install pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a95Nat7aJw4i"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj57seGtJwNH",
        "outputId": "4c9e2dde-4ee1-4050-d683-1329d7afa54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_HIgjNdQ1qF"
      },
      "source": [
        "# **1DCNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcfSISnPJYpd"
      },
      "source": [
        "# Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hfi1fDpuMqs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0qyokpHJisK"
      },
      "source": [
        "# Define Directory and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpzHgP8HJf-w"
      },
      "outputs": [],
      "source": [
        "# Directory containing 50 folders for respective speakers\n",
        "data_path = '/content/drive/MyDrive/archive (14)/50_speakers_audio_data'\n",
        "\n",
        "# Hyperparameters\n",
        "n_mfcc = 13         # Number of MFCC features\n",
        "max_pad_len = 100   # Pad or truncate MFCCs to this length\n",
        "batch_size = 64     # Optimized batch size for batch processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqavX5TtJnNR"
      },
      "source": [
        "# MFCC Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r14w-0SpJqin"
      },
      "outputs": [],
      "source": [
        "def extract_mfcc_features(audio, sample_rate, n_mfcc=n_mfcc, max_pad_len=max_pad_len):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    if mfccs is None or len(mfccs) == 0:\n",
        "        print(\"Warning: MFCC extraction returned None or empty array.\")\n",
        "        return None\n",
        "    mfccs = np.pad(mfccs, ((0, 0), (0, max(0, max_pad_len - mfccs.shape[1]))), mode='constant')\n",
        "    return mfccs[:, :max_pad_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYzChOtPJvzA"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHKySx2CJu3S"
      },
      "outputs": [],
      "source": [
        "def augment_audio(audio):\n",
        "    speed_change = np.random.uniform(0.9, 1.1)\n",
        "    augmented_audio = librosa.effects.time_stretch(audio, rate=speed_change)\n",
        "    noise = np.random.randn(len(augmented_audio)) * 0.005\n",
        "    augmented_audio += noise\n",
        "    return augmented_audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjQrpeh2LTiq"
      },
      "source": [
        "# Load data and extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07HL8IQtK-Zu"
      },
      "outputs": [],
      "source": [
        "features = []\n",
        "labels = []\n",
        "\n",
        "for speaker in os.listdir(data_path):\n",
        "    speaker_folder = os.path.join(data_path, speaker)\n",
        "    if os.path.isdir(speaker_folder):\n",
        "        for file_name in os.listdir(speaker_folder):\n",
        "            file_path = os.path.join(speaker_folder, file_name)\n",
        "            audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "            if audio is None or len(audio) == 0:\n",
        "                print(f\"Warning: Audio file {file_path} could not be loaded.\")\n",
        "                continue\n",
        "            mfcc_features = extract_mfcc_features(audio, sample_rate)\n",
        "            if mfcc_features is not None:\n",
        "                features.append(mfcc_features)\n",
        "                labels.append(speaker)\n",
        "\n",
        "                # Augment and extract features for the augmented audio\n",
        "                augmented_audio = augment_audio(audio)\n",
        "                augmented_mfcc = extract_mfcc_features(augmented_audio, sample_rate)\n",
        "                if augmented_mfcc is not None:\n",
        "                    features.append(augmented_mfcc)\n",
        "                    labels.append(speaker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS--u1IYQ0ld"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4V3FKJBQ22N"
      },
      "outputs": [],
      "source": [
        "# Convert features and labels to numpy arrays\n",
        "X = np.array(features)\n",
        "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83NpQydQ6ke"
      },
      "source": [
        "# Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN7VYfxqQ87l"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(n_mfcc, max_pad_len, 1)),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (2, 2), padding='same', activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE8C6ru5Q-RO"
      },
      "source": [
        "# Compile and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siD-74syRCCj",
        "outputId": "e5e9635f-60f2-48f2-e26a-043cac53f948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 243ms/step - accuracy: 0.0961 - loss: 3.5952 - val_accuracy: 0.1512 - val_loss: 3.3519 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 239ms/step - accuracy: 0.2875 - loss: 2.5973 - val_accuracy: 0.3065 - val_loss: 2.6096 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 240ms/step - accuracy: 0.4260 - loss: 2.0365 - val_accuracy: 0.4488 - val_loss: 2.1142 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 239ms/step - accuracy: 0.5282 - loss: 1.6138 - val_accuracy: 0.6050 - val_loss: 1.4654 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 238ms/step - accuracy: 0.6364 - loss: 1.2684 - val_accuracy: 0.6756 - val_loss: 1.2012 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 239ms/step - accuracy: 0.6630 - loss: 1.1005 - val_accuracy: 0.7493 - val_loss: 0.9110 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 235ms/step - accuracy: 0.7212 - loss: 0.9411 - val_accuracy: 0.7214 - val_loss: 0.9364 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 242ms/step - accuracy: 0.7350 - loss: 0.8909 - val_accuracy: 0.7383 - val_loss: 0.8365 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.7631 - loss: 0.7775 - val_accuracy: 0.8030 - val_loss: 0.6471 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 240ms/step - accuracy: 0.7823 - loss: 0.7030 - val_accuracy: 0.7940 - val_loss: 0.6019 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 236ms/step - accuracy: 0.7996 - loss: 0.6427 - val_accuracy: 0.7841 - val_loss: 0.6590 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 245ms/step - accuracy: 0.8160 - loss: 0.5838 - val_accuracy: 0.8358 - val_loss: 0.5226 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 228ms/step - accuracy: 0.8164 - loss: 0.5864 - val_accuracy: 0.8169 - val_loss: 0.5676 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 240ms/step - accuracy: 0.8241 - loss: 0.5238 - val_accuracy: 0.8318 - val_loss: 0.5079 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 290ms/step - accuracy: 0.8381 - loss: 0.5105 - val_accuracy: 0.7930 - val_loss: 0.6384 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 233ms/step - accuracy: 0.8581 - loss: 0.4706 - val_accuracy: 0.8517 - val_loss: 0.4563 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 225ms/step - accuracy: 0.8485 - loss: 0.4566 - val_accuracy: 0.8756 - val_loss: 0.3808 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 235ms/step - accuracy: 0.8636 - loss: 0.4377 - val_accuracy: 0.8448 - val_loss: 0.4463 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.8596 - loss: 0.4329 - val_accuracy: 0.8647 - val_loss: 0.3884 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 235ms/step - accuracy: 0.8676 - loss: 0.3942 - val_accuracy: 0.8836 - val_loss: 0.3694 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.8804 - loss: 0.3664 - val_accuracy: 0.8587 - val_loss: 0.4021 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 224ms/step - accuracy: 0.8835 - loss: 0.3625 - val_accuracy: 0.8965 - val_loss: 0.3080 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.8863 - loss: 0.3483 - val_accuracy: 0.8955 - val_loss: 0.3182 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 234ms/step - accuracy: 0.8762 - loss: 0.3521 - val_accuracy: 0.8507 - val_loss: 0.4314 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 232ms/step - accuracy: 0.8907 - loss: 0.3196 - val_accuracy: 0.8617 - val_loss: 0.4001 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 231ms/step - accuracy: 0.9109 - loss: 0.2826 - val_accuracy: 0.9065 - val_loss: 0.2796 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.9229 - loss: 0.2404 - val_accuracy: 0.9154 - val_loss: 0.2583 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.9228 - loss: 0.2573 - val_accuracy: 0.9045 - val_loss: 0.2750 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 237ms/step - accuracy: 0.9202 - loss: 0.2548 - val_accuracy: 0.9124 - val_loss: 0.2502 - learning_rate: 5.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 236ms/step - accuracy: 0.9359 - loss: 0.2246 - val_accuracy: 0.9284 - val_loss: 0.2305 - learning_rate: 5.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 226ms/step - accuracy: 0.9309 - loss: 0.2250 - val_accuracy: 0.8985 - val_loss: 0.2721 - learning_rate: 5.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 262ms/step - accuracy: 0.9362 - loss: 0.2119 - val_accuracy: 0.9025 - val_loss: 0.2754 - learning_rate: 5.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 234ms/step - accuracy: 0.9353 - loss: 0.2154 - val_accuracy: 0.9174 - val_loss: 0.2497 - learning_rate: 5.0000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 224ms/step - accuracy: 0.9497 - loss: 0.1797 - val_accuracy: 0.9294 - val_loss: 0.2261 - learning_rate: 2.5000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 234ms/step - accuracy: 0.9428 - loss: 0.1878 - val_accuracy: 0.9224 - val_loss: 0.2353 - learning_rate: 2.5000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.9416 - loss: 0.1957 - val_accuracy: 0.9303 - val_loss: 0.2181 - learning_rate: 2.5000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 235ms/step - accuracy: 0.9388 - loss: 0.2013 - val_accuracy: 0.9244 - val_loss: 0.2244 - learning_rate: 2.5000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 222ms/step - accuracy: 0.9490 - loss: 0.1708 - val_accuracy: 0.9274 - val_loss: 0.2216 - learning_rate: 2.5000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 237ms/step - accuracy: 0.9437 - loss: 0.1793 - val_accuracy: 0.9264 - val_loss: 0.2145 - learning_rate: 2.5000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 233ms/step - accuracy: 0.9466 - loss: 0.1707 - val_accuracy: 0.9274 - val_loss: 0.2213 - learning_rate: 2.5000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 228ms/step - accuracy: 0.9522 - loss: 0.1820 - val_accuracy: 0.9323 - val_loss: 0.2169 - learning_rate: 2.5000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 235ms/step - accuracy: 0.9514 - loss: 0.1668 - val_accuracy: 0.9184 - val_loss: 0.2194 - learning_rate: 2.5000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 233ms/step - accuracy: 0.9582 - loss: 0.1555 - val_accuracy: 0.9333 - val_loss: 0.2024 - learning_rate: 1.2500e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 238ms/step - accuracy: 0.9580 - loss: 0.1582 - val_accuracy: 0.9323 - val_loss: 0.2083 - learning_rate: 1.2500e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.9549 - loss: 0.1599 - val_accuracy: 0.9353 - val_loss: 0.2007 - learning_rate: 1.2500e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 221ms/step - accuracy: 0.9549 - loss: 0.1645 - val_accuracy: 0.9383 - val_loss: 0.2010 - learning_rate: 1.2500e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 222ms/step - accuracy: 0.9529 - loss: 0.1516 - val_accuracy: 0.9323 - val_loss: 0.2070 - learning_rate: 1.2500e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 274ms/step - accuracy: 0.9626 - loss: 0.1382 - val_accuracy: 0.9393 - val_loss: 0.1954 - learning_rate: 1.2500e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 235ms/step - accuracy: 0.9588 - loss: 0.1450 - val_accuracy: 0.9294 - val_loss: 0.2070 - learning_rate: 1.2500e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 225ms/step - accuracy: 0.9583 - loss: 0.1508 - val_accuracy: 0.9313 - val_loss: 0.2001 - learning_rate: 1.2500e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.9508 - loss: 0.1637 - val_accuracy: 0.9264 - val_loss: 0.2060 - learning_rate: 1.2500e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.9563 - loss: 0.1528 - val_accuracy: 0.9333 - val_loss: 0.1955 - learning_rate: 6.2500e-05\n",
            "Epoch 53/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.9536 - loss: 0.1523 - val_accuracy: 0.9383 - val_loss: 0.1924 - learning_rate: 6.2500e-05\n",
            "Epoch 54/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 235ms/step - accuracy: 0.9586 - loss: 0.1458 - val_accuracy: 0.9383 - val_loss: 0.1948 - learning_rate: 6.2500e-05\n",
            "Epoch 55/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 228ms/step - accuracy: 0.9561 - loss: 0.1414 - val_accuracy: 0.9383 - val_loss: 0.1943 - learning_rate: 6.2500e-05\n",
            "Epoch 56/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 231ms/step - accuracy: 0.9629 - loss: 0.1317 - val_accuracy: 0.9373 - val_loss: 0.1926 - learning_rate: 6.2500e-05\n",
            "Epoch 57/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 235ms/step - accuracy: 0.9588 - loss: 0.1374 - val_accuracy: 0.9373 - val_loss: 0.1906 - learning_rate: 3.1250e-05\n",
            "Epoch 58/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 233ms/step - accuracy: 0.9615 - loss: 0.1384 - val_accuracy: 0.9363 - val_loss: 0.1907 - learning_rate: 3.1250e-05\n",
            "Epoch 59/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 233ms/step - accuracy: 0.9519 - loss: 0.1522 - val_accuracy: 0.9323 - val_loss: 0.1900 - learning_rate: 3.1250e-05\n",
            "Epoch 60/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 238ms/step - accuracy: 0.9689 - loss: 0.1309 - val_accuracy: 0.9343 - val_loss: 0.1915 - learning_rate: 3.1250e-05\n",
            "Epoch 61/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 236ms/step - accuracy: 0.9699 - loss: 0.1176 - val_accuracy: 0.9353 - val_loss: 0.1901 - learning_rate: 3.1250e-05\n",
            "Epoch 62/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 226ms/step - accuracy: 0.9625 - loss: 0.1353 - val_accuracy: 0.9363 - val_loss: 0.1902 - learning_rate: 3.1250e-05\n",
            "Epoch 63/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.9593 - loss: 0.1420 - val_accuracy: 0.9373 - val_loss: 0.1918 - learning_rate: 1.5625e-05\n",
            "Epoch 64/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 235ms/step - accuracy: 0.9618 - loss: 0.1377 - val_accuracy: 0.9383 - val_loss: 0.1929 - learning_rate: 1.5625e-05\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for training\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test), callbacks=[early_stopping, lr_scheduler])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyjCP7WTREap"
      },
      "source": [
        "# Evaluate and Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e63plp8aRG6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432b44a1-e296-4953-8a3f-329a4e623389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9371 - loss: 0.1620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.23%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the model and the Label Encoder\n",
        "model.save('speaker_identification_cnn_model.h5')\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3o3w5ikRKCj"
      },
      "source": [
        "# Prediction function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHswS6vLRMLv"
      },
      "outputs": [],
      "source": [
        "def predict_speaker(file_path):\n",
        "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "    if audio is None or len(audio) == 0:\n",
        "        print(f\"Warning: Audio file {file_path} could not be loaded.\")\n",
        "        return None\n",
        "    mfcc_features = extract_mfcc_features(audio, sample_rate)\n",
        "    if mfcc_features is None:\n",
        "        return None\n",
        "    mfcc_features = np.reshape(mfcc_features, (1, mfcc_features.shape[0], mfcc_features.shape[1], 1))\n",
        "    prediction = model.predict(mfcc_features)\n",
        "    speaker_label = np.argmax(prediction)\n",
        "    speaker_name = label_encoder.inverse_transform([speaker_label])[0]\n",
        "    return speaker_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZzcQOLqROIH"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BQHjUDlRQwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec903075-177b-4263-fd3f-c486a86bec49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "The identified speaker is: Speaker0026\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    test_audio_path = '/content/drive/MyDrive/archive (14)/50_speakers_audio_data/Speaker0026/Speaker26_000.wav'\n",
        "    identified_speaker = predict_speaker(test_audio_path)\n",
        "    print(f\"The identified speaker is: {identified_speaker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwvUN63tVvbn"
      },
      "source": [
        "# **GMM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxOhSxFgPYtc",
        "outputId": "1e523dab-17ed-40a5-baaf-e3ed0906eb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.04%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "# Directory containing folders for respective speakers\n",
        "data_path = '/content/drive/MyDrive/archive (14)/50_speakers_audio_data'\n",
        "\n",
        "# Hyperparameters\n",
        "n_mfcc = 13      # Number of MFCC features\n",
        "max_pad_len = 100  # Pad or truncate MFCCs to this length\n",
        "n_components = 16  # Number of components for each speaker's GMM\n",
        "\n",
        "# Function to load and preprocess audio files\n",
        "def extract_mfcc_features(audio, sample_rate, n_mfcc=n_mfcc, max_pad_len=max_pad_len):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    if mfccs is None or len(mfccs) == 0:\n",
        "        print(\"Warning: MFCC extraction returned None or empty array.\")\n",
        "        return None\n",
        "    mfccs = np.pad(mfccs, ((0, 0), (0, max(0, max_pad_len - mfccs.shape[1]))), mode='constant')\n",
        "    return mfccs[:, :max_pad_len].T  # Transpose for GMM compatibility\n",
        "\n",
        "# Prepare data and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for speaker in os.listdir(data_path):\n",
        "    speaker_folder = os.path.join(data_path, speaker)\n",
        "    if os.path.isdir(speaker_folder):\n",
        "        for file_name in os.listdir(speaker_folder):\n",
        "            file_path = os.path.join(speaker_folder, file_name)\n",
        "            audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "            if audio is None or len(audio) == 0:\n",
        "                print(f\"Warning: Audio file {file_path} could not be loaded.\")\n",
        "                continue\n",
        "            mfcc_features = extract_mfcc_features(audio, sample_rate)\n",
        "            if mfcc_features is not None:\n",
        "                features.append(mfcc_features)\n",
        "                labels.append(speaker)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)\n",
        "speakers = label_encoder.classes_\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train GMM for each speaker on training data\n",
        "gmm_models = {}\n",
        "for speaker_index, speaker_name in enumerate(speakers):\n",
        "    # Collect all MFCC features for the current speaker in the training set\n",
        "    speaker_features = np.vstack([X_train[i] for i in range(len(X_train)) if y_train[i] == speaker_index])\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=200, random_state=42)\n",
        "    gmm.fit(speaker_features)\n",
        "    gmm_models[speaker_name] = gmm\n",
        "\n",
        "# Save the GMM models and label encoder for future use\n",
        "with open('gmm_models.pkl', 'wb') as f:\n",
        "    pickle.dump(gmm_models, f)\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# Function to predict speaker for a new audio file\n",
        "def predict_speaker(mfcc_features, gmm_models):\n",
        "    log_likelihoods = {speaker: gmm.score(mfcc_features) for speaker, gmm in gmm_models.items()}\n",
        "    return max(log_likelihoods, key=log_likelihoods.get)\n",
        "\n",
        "# Evaluate the model\n",
        "correct_predictions = 0\n",
        "for i in range(len(X_test)):\n",
        "    mfcc_features = X_test[i]\n",
        "    true_speaker = label_encoder.inverse_transform([y_test[i]])[0]\n",
        "    predicted_speaker = predict_speaker(mfcc_features, gmm_models)\n",
        "\n",
        "    if predicted_speaker == true_speaker:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / len(X_test) * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1cHlonnVWmF",
        "outputId": "ab3fcce0-bb65-4c18-e861-8371134dd963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The identified speaker for the given audio file is: Speaker0026\n"
          ]
        }
      ],
      "source": [
        "def predict_speaker_from_audio(file_path, gmm_models, label_encoder):\n",
        "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "    if audio is None or len(audio) == 0:\n",
        "        print(f\"Warning: Audio file {file_path} could not be loaded.\")\n",
        "        return None\n",
        "    mfcc_features = extract_mfcc_features(audio, sample_rate)\n",
        "    if mfcc_features is None:\n",
        "        return None\n",
        "\n",
        "    # Compute the average log likelihood for each model\n",
        "    log_likelihoods = {speaker: gmm.score(mfcc_features) for speaker, gmm in gmm_models.items()}\n",
        "    predicted_speaker = max(log_likelihoods, key=log_likelihoods.get)\n",
        "    return predicted_speaker\n",
        "\n",
        "# Example usage of predicting speaker from a single audio file\n",
        "if __name__ == \"__main__\":\n",
        "    test_audio_path = '/content/drive/MyDrive/archive (14)/50_speakers_audio_data/Speaker0026/Speaker26_000.wav'\n",
        "    identified_speaker = predict_speaker_from_audio(test_audio_path, gmm_models, label_encoder)\n",
        "    print(f\"The identified speaker for the given audio file is: {identified_speaker}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}